{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "# run that setup file to avoid errors\n",
    "\n",
    "import gym_gazebo\n",
    "import time\n",
    "import numpy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospy\n",
    "#import tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from queue import PriorityQueue\n",
    "\n",
    "# from model import QNetwork\n",
    "# from model import QNetworkCNN\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import deque\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gazebo launched!\n",
      "Unable to register with master node [http://localhost:11311]: master may not be running yet. Will keep trying.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('GazeboBATBot_Sonar-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 1])\n",
      "out size torch.Size([1, 100, 4])\n",
      "covolutional layer Conv1d(100, 100, kernel_size=(8,), stride=(1,), padding=(5,))\n"
     ]
    }
   ],
   "source": [
    "# checking the working of 1D conv \n",
    "a = torch.randn(1, 100, 1)  \n",
    "\n",
    "print(a.size())\n",
    "m = nn.Conv1d(100, 100, kernel_size = 8 , stride = 1 , padding = 5 ) \n",
    "out = m(a)\n",
    "print(\"out size\",out.size())\n",
    "print(\"covolutional layer\",m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caplab/Batbots/gymBatbot/gym_gazebo_BatBots/gym_gazebo/envs/installation/catkin_ws/src/batbot_simulator/mybot_sonar/src/mybot_sonar/Acoustics.py:71: UserWarning: db2pa function should not be used with negative values!\n",
      "  if min_value < 0: warnings.warn(\"db2pa function should not be used with negative values!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting echoes...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo energy before 45.340723846549125 24.623326996216548\n",
      "plotting echoes...\n",
      "l e 33.594346994051364  R E 31.682921654073976 energies 220.03949185157114 57.425145392898074\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug 2\n",
      "debug\n",
      "the observation size 2 the reduced observation (33.594346994051364, 31.682921654073976)\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "observation, reward, done, info = env.step(action, debug = True)\n",
    "print(\"the observation size\", len(observation), \"the reduced observation\", observation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def print_slow(str , delay = 0.1):\n",
    "    for letter in str:\n",
    "        sys.stdout.write(letter)\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aye aye aye enga paru"
     ]
    }
   ],
   "source": [
    "print_slow(\"aye aye aye enga paru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the state_type of the observation <class 'tuple'> (33.594346994051364, 31.682921654073976)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state_Type = type(observation)\n",
    "print(\"the state_type of the observation\", state_Type , observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkCNN_1D_BATBOT(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64 , dropout = 0.1 , augment_frames = 3 ):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetworkCNN_1D_BATBOT, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed = seed)\n",
    "        self.conv1_left = nn.Conv1d(augment_frames, 128, kernel_size = 128 , stride=5, padding=1)\n",
    "        self.conv1bnorm_left = nn.BatchNorm1d(128)\n",
    "        self.conv2_left = nn.Conv1d(128, 64, kernel_size = 16, stride=2, padding=1)\n",
    "        self.conv2bnorm_left = nn.BatchNorm1d(64)\n",
    "        self.conv3_left = nn.Conv1d(64, 64, kernel_size = 5, stride=1, padding=1)\n",
    "        self.conv3bnorm_left = nn.BatchNorm1d(64)\n",
    "        self.dropout_left = nn.Dropout(dropout)\n",
    "        self.pool_left = nn.MaxPool1d(kernel_size = 4, stride = 4 )\n",
    "        \n",
    "        self.conv1_right = nn.Conv1d(augment_frames, 128, kernel_size= 128, stride=5, padding=1)\n",
    "        self.conv1bnorm_right = nn.BatchNorm1d(128)\n",
    "        self.conv2_right = nn.Conv1d(128, 64, kernel_size = 16, stride=2, padding=1)\n",
    "        self.conv2bnorm_right = nn.BatchNorm1d(64)\n",
    "        self.conv3_right = nn.Conv1d(64, 64, kernel_size = 5, stride=1, padding=1)\n",
    "        self.conv3bnorm_right = nn.BatchNorm1d(64)\n",
    "        self.dropout_right = nn.Dropout(dropout)\n",
    "        self.pool_right = nn.MaxPool1d(kernel_size = 4, stride = 4 )\n",
    "        \n",
    "        # this needs to be changed accordingly\n",
    "        self.in_linear = 6*64\n",
    "        \n",
    "        self.fc1 = nn.Linear(2*self.in_linear, fc1_units)\n",
    "        self.fc1bnorm = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units,  fc2_units)\n",
    "        self.fc2bnorm = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc2d = nn.Linear(fc2_units,  fc2_units)\n",
    "        self.fc2dbnorm = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "    def forward(self, left_state, right_state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x1 = self.conv1_left(left_state)\n",
    "        x1 = self.conv1bnorm_left(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.pool_left(x1)\n",
    "        x1 = self.conv2_left(x1)\n",
    "        x1 = self.conv2bnorm_left(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.pool_left(x1)\n",
    "        x1 = self.conv3_left(x1)\n",
    "        x1 = self.conv3bnorm_left(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.pool_left(x1)\n",
    "        \n",
    "        x2 = self.conv1_right(right_state)\n",
    "        x2 = self.conv1bnorm_right(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.pool_right(x2)\n",
    "        x2 = self.conv2_right(x2)\n",
    "        x2 = self.conv2bnorm_right(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.pool_right(x2)\n",
    "        x2 = self.conv3_right(x2)\n",
    "        x2 = self.conv3bnorm_right(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.pool_right(x2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(x.size())\n",
    "        x1 = x1.view(-1,self.in_linear)\n",
    "        x2 = x2.view(-1,self.in_linear)\n",
    "        x = torch.cat((x1,x2)).view(-1, 2*self.in_linear)\n",
    "        #print(x.size())\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1bnorm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2bnorm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2d(x)\n",
    "        x = self.fc2dbnorm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this uses the lower diemensional data \n",
    "# energies of the echo from the left and right hear \n",
    "class QNetworkANN_BATBOT(nn.Module):\n",
    "    def __init__(self, state_size , action_size , seed, fc1_units=64, fc2_units=64 ,fc3_units = 64, dropout = 0.3, augment_frames = 3):\n",
    "        super(QNetworkANN_BATBOT, self).__init__()\n",
    "        self.in_linear = 2*augment_frames\n",
    "        # LAYER 1 - input layer\n",
    "        self.fc1 = nn.Linear(self.in_linear , fc1_units)\n",
    "        self.fc1bnorm = nn.BatchNorm1d(fc1_units)\n",
    "        # HIDDEN LAYER 1\n",
    "        self.fc2 = nn.Linear(fc1_units,fc2_units)\n",
    "        self.fc2bnorm = nn.BatchNorm1d(fc2_units)\n",
    "        # HIDDEN LAYER 2\n",
    "        self.fc3 = nn.Linear(fc2_units,fc3_units)\n",
    "        self.fc3bnorm = nn.BatchNorm1d(fc3_units)\n",
    "        # OUTPUT LAYER\n",
    "        self.fc4 = nn.Linear(fc3_units, action_size)\n",
    "        #DROPOUT\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "    def forward(self, left_state , right_state):\n",
    "        #joining the left ear and right ear\n",
    "        x = torch.cat((left_state,right_state)).view(-1, self.in_linear)\n",
    "        #INPUT BLOCK\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1bnorm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        #LAYEr 1 BLOCK\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2bnorm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        #LAYER 2 BLOCK\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc3bnorm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # OUTPUT BLOCK\n",
    "        return self.fc4(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this uses the lower diemensional data \n",
    "# energies of the echo from the left and right hear \n",
    "class QNetworkANN(nn.Module):\n",
    "    def __init__(self, state_size , action_size , seed, fc1_units=64, fc2_units=64 ,fc3_units = 64, dropout = 0.3, augment_frames = 3):\n",
    "        super(QNetworkANN, self).__init__()\n",
    "        self.in_linear = state_size*augment_frames\n",
    "        # LAYER 1 - input layer\n",
    "        self.fc1 = nn.Linear(self.in_linear , fc1_units)\n",
    "        self.fc1bnorm = nn.BatchNorm1d(fc1_units)\n",
    "        # HIDDEN LAYER 1\n",
    "        self.fc2 = nn.Linear(fc1_units,fc2_units)\n",
    "        self.fc2bnorm = nn.BatchNorm1d(fc2_units)\n",
    "        # HIDDEN LAYER 2\n",
    "        self.fc3 = nn.Linear(fc2_units,fc3_units)\n",
    "        self.fc3bnorm = nn.BatchNorm1d(fc3_units)\n",
    "        # OUTPUT LAYER\n",
    "        self.fc4 = nn.Linear(fc3_units, action_size)\n",
    "        #DROPOUT\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "    def forward(self, state):\n",
    "        #joining the left ear and right ear\n",
    "        x = state.view(-1, self.in_linear)\n",
    "        #INPUT BLOCK\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1bnorm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        #LAYEr 1 BLOCK\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2bnorm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        #LAYER 2 BLOCK\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc3bnorm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # OUTPUT BLOCK\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(6e5)  # replay buffer size\n",
    "BATCH_SIZE = 128       # minibatch size\n",
    "GAMMA = 0.99           # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 1e-4               # learning rate \n",
    "\n",
    "WeightDecay = 0\n",
    "\n",
    "UPDATE_EVERY = 1      # how often to update the network\n",
    "N_Update_per = 5 # no of updates per time\n",
    "\n",
    "Momentum = 0.9  \n",
    "Epislon = 0.99\n",
    "min_Epislon = 0.001\n",
    "\n",
    "Decay = 0.99\n",
    "MaxBeta = 1\n",
    "MinBeta = 0\n",
    "\n",
    "\n",
    "ALPHA = 0.5\n",
    "ALPHA2 = 1\n",
    "ALPHA_min = 0.01\n",
    "Alphadecay = 0.999 \n",
    "ErrorOffset = 0.01\n",
    "min_beta = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -0.18000000000000002\n"
     ]
    }
   ],
   "source": [
    "action_size = 3\n",
    "score = 0 # initialize the score\n",
    "i = 30\n",
    "while i > 0:\n",
    "    action = np.random.randint(action_size)                 # select an action\n",
    "    observation, reward, done, info = env.step(action)      # send the action to the environment\n",
    "    next_state = observation                                # see if episode has finished\n",
    "    \n",
    "    \n",
    "    score += reward                                         # update the score\n",
    "    state = next_state                                      # roll over the state to next time step\n",
    "    i-=1\n",
    "    if done:                                                # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😅\n"
     ]
    }
   ],
   "source": [
    "print(\"\\U0001F605\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from queue import PriorityQueue\n",
    "\n",
    "# from model import QNetwork\n",
    "# from model import QNetworkCNN\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89.80701862567372, 88.75853996082054)\n"
     ]
    }
   ],
   "source": [
    "print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "AI: Wtf! r u doing 😠 I am dying here... \n",
      "ME: Sorry 😅 zoned out little .. was watching some lame anime.. 😬 \n",
      "AI: Adai Venna !!!.... mudidu vellaiya paru... \n",
      "ME: Seringa.. 😬 \n",
      "the model is defined successfully QNetworkANN_BATBOT(\n",
      "  (fc1): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (fc1bnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc2bnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3bnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "test_memory added to the replay buffer 800 batch size 128\n",
      "Experience(state=[0, 126961361.25741975], action=2, reward=1, next_state=[1, 126961361.25741975], done=False)\n",
      "sample (tensor([[[3.5900e+02, 1.2696e+08],\n",
      "         [3.6000e+02, 1.2696e+08],\n",
      "         [3.6100e+02, 1.2696e+08]],\n",
      "\n",
      "        [[3.3000e+01, 1.2696e+08],\n",
      "         [3.4000e+01, 1.2696e+08],\n",
      "         [3.5000e+01, 1.2696e+08]],\n",
      "\n",
      "        [[6.6700e+02, 1.2696e+08],\n",
      "         [6.6800e+02, 1.2696e+08],\n",
      "         [6.6900e+02, 1.2696e+08]],\n",
      "\n",
      "        [[2.0400e+02, 1.2696e+08],\n",
      "         [2.0500e+02, 1.2696e+08],\n",
      "         [2.0600e+02, 1.2696e+08]],\n",
      "\n",
      "        [[6.7900e+02, 1.2696e+08],\n",
      "         [6.8000e+02, 1.2696e+08],\n",
      "         [6.8100e+02, 1.2696e+08]],\n",
      "\n",
      "        [[7.8300e+02, 1.2696e+08],\n",
      "         [7.8400e+02, 1.2696e+08],\n",
      "         [7.8500e+02, 1.2696e+08]],\n",
      "\n",
      "        [[5.0100e+02, 1.2696e+08],\n",
      "         [5.0200e+02, 1.2696e+08],\n",
      "         [5.0300e+02, 1.2696e+08]],\n",
      "\n",
      "        [[4.2800e+02, 1.2696e+08],\n",
      "         [4.2900e+02, 1.2696e+08],\n",
      "         [4.3000e+02, 1.2696e+08]],\n",
      "\n",
      "        [[4.8700e+02, 1.2696e+08],\n",
      "         [4.8800e+02, 1.2696e+08],\n",
      "         [4.8900e+02, 1.2696e+08]],\n",
      "\n",
      "        [[5.5500e+02, 1.2696e+08],\n",
      "         [5.5600e+02, 1.2696e+08],\n",
      "         [5.5700e+02, 1.2696e+08]]], device='cuda:0'), tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0'), tensor([[[3.6000e+02, 1.2696e+08],\n",
      "         [3.6100e+02, 1.2696e+08],\n",
      "         [3.6200e+02, 1.2696e+08]],\n",
      "\n",
      "        [[3.4000e+01, 1.2696e+08],\n",
      "         [3.5000e+01, 1.2696e+08],\n",
      "         [3.6000e+01, 1.2696e+08]],\n",
      "\n",
      "        [[6.6800e+02, 1.2696e+08],\n",
      "         [6.6900e+02, 1.2696e+08],\n",
      "         [6.7000e+02, 1.2696e+08]],\n",
      "\n",
      "        [[2.0500e+02, 1.2696e+08],\n",
      "         [2.0600e+02, 1.2696e+08],\n",
      "         [2.0700e+02, 1.2696e+08]],\n",
      "\n",
      "        [[6.8000e+02, 1.2696e+08],\n",
      "         [6.8100e+02, 1.2696e+08],\n",
      "         [6.8200e+02, 1.2696e+08]],\n",
      "\n",
      "        [[7.8400e+02, 1.2696e+08],\n",
      "         [7.8500e+02, 1.2696e+08],\n",
      "         [7.8600e+02, 1.2696e+08]],\n",
      "\n",
      "        [[5.0200e+02, 1.2696e+08],\n",
      "         [5.0300e+02, 1.2696e+08],\n",
      "         [5.0400e+02, 1.2696e+08]],\n",
      "\n",
      "        [[4.2900e+02, 1.2696e+08],\n",
      "         [4.3000e+02, 1.2696e+08],\n",
      "         [4.3100e+02, 1.2696e+08]],\n",
      "\n",
      "        [[4.8800e+02, 1.2696e+08],\n",
      "         [4.8900e+02, 1.2696e+08],\n",
      "         [4.9000e+02, 1.2696e+08]],\n",
      "\n",
      "        [[5.5600e+02, 1.2696e+08],\n",
      "         [5.5700e+02, 1.2696e+08],\n",
      "         [5.5800e+02, 1.2696e+08]]], device='cuda:0'), tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')) states torch.Size([10, 3, 2])\n",
      "torch.Size([10, 3]) tensor([[359., 360., 361.],\n",
      "        [ 33.,  34.,  35.],\n",
      "        [667., 668., 669.],\n",
      "        [204., 205., 206.],\n",
      "        [679., 680., 681.],\n",
      "        [783., 784., 785.],\n",
      "        [501., 502., 503.],\n",
      "        [428., 429., 430.],\n",
      "        [487., 488., 489.],\n",
      "        [555., 556., 557.]], device='cuda:0') tensor([[1.2696e+08, 1.2696e+08, 1.2696e+08],\n",
      "        [1.2696e+08, 1.2696e+08, 1.2696e+08],\n",
      "        [1.2696e+08, 1.2696e+08, 1.2696e+08],\n",
      "        [1.2696e+08, 1.2696e+08, 1.2696e+08],\n",
      "        [1.2696e+08, 1.2696e+08, 1.2696e+08],\n",
      "        [1.2696e+08, 1.2696e+08, 1.2696e+08],\n",
      "        [1.2696e+08, 1.2696e+08, 1.2696e+08],\n",
      "        [1.2696e+08, 1.2696e+08, 1.2696e+08],\n",
      "        [1.2696e+08, 1.2696e+08, 1.2696e+08],\n",
      "        [1.2696e+08, 1.2696e+08, 1.2696e+08]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(type(observation))\n",
    "state_Type = type(observation)\n",
    "test_agent= Agent(len(observation), 3 , state_Type , seed = 0 , signal = \"eng\")\n",
    "test_agent.memory.batch_size = 128\n",
    "for i in range(800):\n",
    "    test_agent.step([i,observation[1]], 2, 1, [i+1,observation[1]], False)\n",
    "print(\"test_memory added to the replay buffer\" , len(test_agent.memory.memory), \"batch size\", test_agent.memory.batch_size)\n",
    "print(test_agent.memory.memory[0])\n",
    "test_sample = test_agent.memory.sample_idx_withBatch(10)\n",
    "print(\"sample\",test_sample, \"states\", test_sample[0].size())\n",
    "states = test_sample[0]\n",
    "left_s = states[:,:,0].view(10 ,  -1)\n",
    "right_s = states[:,:,1].view(10 ,  -1)\n",
    "print(left_s.size(), left_s , right_s)\n",
    "# print(\"the sampled memory for learning\", np.shape(test_agent.memory.sample_idx()[0]), test_agent.memory.sample_idx()[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3.5900e+02, 1.2696e+08],\n",
      "         [3.6000e+02, 1.2696e+08],\n",
      "         [3.6100e+02, 1.2696e+08]],\n",
      "\n",
      "        [[3.3000e+01, 1.2696e+08],\n",
      "         [3.4000e+01, 1.2696e+08],\n",
      "         [3.5000e+01, 1.2696e+08]],\n",
      "\n",
      "        [[6.6700e+02, 1.2696e+08],\n",
      "         [6.6800e+02, 1.2696e+08],\n",
      "         [6.6900e+02, 1.2696e+08]],\n",
      "\n",
      "        [[2.0400e+02, 1.2696e+08],\n",
      "         [2.0500e+02, 1.2696e+08],\n",
      "         [2.0600e+02, 1.2696e+08]],\n",
      "\n",
      "        [[6.7900e+02, 1.2696e+08],\n",
      "         [6.8000e+02, 1.2696e+08],\n",
      "         [6.8100e+02, 1.2696e+08]],\n",
      "\n",
      "        [[7.8300e+02, 1.2696e+08],\n",
      "         [7.8400e+02, 1.2696e+08],\n",
      "         [7.8500e+02, 1.2696e+08]],\n",
      "\n",
      "        [[5.0100e+02, 1.2696e+08],\n",
      "         [5.0200e+02, 1.2696e+08],\n",
      "         [5.0300e+02, 1.2696e+08]],\n",
      "\n",
      "        [[4.2800e+02, 1.2696e+08],\n",
      "         [4.2900e+02, 1.2696e+08],\n",
      "         [4.3000e+02, 1.2696e+08]],\n",
      "\n",
      "        [[4.8700e+02, 1.2696e+08],\n",
      "         [4.8800e+02, 1.2696e+08],\n",
      "         [4.8900e+02, 1.2696e+08]],\n",
      "\n",
      "        [[5.5500e+02, 1.2696e+08],\n",
      "         [5.5600e+02, 1.2696e+08],\n",
      "         [5.5700e+02, 1.2696e+08]]], device='cuda:0') torch.Size([10, 3, 2]) torch.Size([10, 6])\n"
     ]
    }
   ],
   "source": [
    "print(states, states.size(), states.view(10,-1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb8a19fda90>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a  = test_agent.memory.sample_idx(peek_a_boo= True)\n",
    "print(a[0][:,0,0].shape)\n",
    "plt.plot(a[0][0,2,1].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-b0aab9432be0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the test agent action selection is passed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples. \n",
    "    Also can be used to store the priortized tuples\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            Priority(float):\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.selectedIdx = np.array([])\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        #print(experiences[0])\n",
    "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None] , axis =0 )).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None], axis =0)).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    def sample_idx(self , peek_a_boo = False):\n",
    "        if peek_a_boo == True:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        index_value = random.sample(list(enumerate(self.memory)), k= batch_size)\n",
    "        #print(\"batch\", self.batch_size)\n",
    "        states = torch.from_numpy(np.stack([[self.memory[e[0]-2].state,self.memory[e[0]-1].state ,e[1].state] for e in index_value if e is not None and e[1].done != True and self.memory[e[0]-1].done != True and e[0]-1 > 0 and e[0] < len(self.memory)-1] , axis = 0 )).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1].action for e in index_value if e is not None and e[1].done != True and self.memory[e[0]-1].done != True and e[0]-1 > 0 and e[0] < len(self.memory)-1])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[1].reward for e in index_value if e is not None and e[1].done != True and self.memory[e[0]-1].done != True and e[0]-1 > 0 and e[0] < len(self.memory)-1])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.stack([[self.memory[e[0]-2].next_state, self.memory[e[0]-1].next_state ,e[1].next_state] for e in index_value if e is not None and e[1].done != True and self.memory[e[0]-1].done != True and e[0]-1 > 0 and e[0] < len(self.memory)-1] , axis = 0 )).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e[1].done for e in index_value if e is not None and e[1].done != True and self.memory[e[0]-1].done != True and e[0]-1 > 0 and e[0] < len(self.memory)-1]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def sample_idx_withBatch(self , batch_size , peek_a_boo = False):\n",
    "        \n",
    "        if peek_a_boo == True:\n",
    "            batch_size = 1\n",
    "        index_value = random.sample(list(enumerate(self.memory)), k=batch_size)\n",
    "        states = torch.from_numpy(np.stack([[self.memory[e[0]-2].state,self.memory[e[0]-1].state ,e[1].state] for e in index_value if e is not None and e[1].done != True and self.memory[e[0]-1].done != True] , axis = 0 )).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1].action for e in index_value if e is not None and e[1].done != True and self.memory[e[0]-1].done != True])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[1].reward for e in index_value if e is not None and e[1].done != True and self.memory[e[0]-1].done != True])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.stack([[self.memory[e[0]-2].next_state,self.memory[e[0]-1].next_state ,e[1].next_state] for e in index_value if e is not None and e[1].done != True and self.memory[e[0]-1].done != True] , axis = 0 )).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e[1].done for e in index_value if e is not None and e[1].done != True and self.memory[e[0]-1].done != True]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "class PriorityReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples. \n",
    "    Also can be used to store the priortized tuples\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, alpha, state_type ,  seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            Priority(float):\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.Prioritymemory = deque(maxlen=buffer_size) \n",
    "        self.batch_size = batch_size\n",
    "        self.expType = [('state',state_type), ('action','float'),('reward','float'), ('next_state',state_type),('done','bool'),('Priority','float')]\n",
    "        self.experienceMemory = np.array([], dtype = self.expType)\n",
    "        self.seed = random.seed(seed)\n",
    "        self.max_size = buffer_size\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done, Priority):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        if(np.size(self.experienceMemory) > self.max_size):\n",
    "            self.experienceMemory = np.delete(self.experienceMemory , 0 )\n",
    "        e = (state, action, reward, next_state, done, Priority)\n",
    "        exp = np.array([e] , dtype = self.expType)\n",
    "        self.experienceMemory = np.append(self.experienceMemory , exp )\n",
    "        #print(\"memory\", np.shape(self.experienceMemory['state'][0]))\n",
    "        #self.memory.append(e)\n",
    "    \n",
    "    def sample(self , alpha , beta):\n",
    "        \"\"\" sample a batch of experiences from memory. based on the Priority\"\"\"\n",
    "        # write ur own implementation\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        if(self.beta < min_beta):\n",
    "            self.beta = min_beta\n",
    "        probs ,Choosenindex = self.Batchsample()\n",
    "        self.selectedIdx = Choosenindex\n",
    "        weights = (np.size(self.experienceMemory)* probs)**(-(self.beta))\n",
    "        weights /= np.max(weights)\n",
    "        weights = Variable(torch.from_numpy(np.vstack(weights)).float().to(device))\n",
    "        states = Variable(torch.from_numpy(np.stack(self.experienceMemory['state'][Choosenindex] , axis =0)).float().to(device))\n",
    "        #print(\"at sampling\",states.size())\n",
    "        actions = Variable(torch.from_numpy(np.vstack(self.experienceMemory['action'][Choosenindex])).long().to(device))\n",
    "        rewards = Variable(torch.from_numpy(np.vstack(self.experienceMemory['reward'][Choosenindex])).float().to(device))\n",
    "        next_states = Variable(torch.from_numpy(np.stack(self.experienceMemory['next_state'][Choosenindex] , axis =0)).float().to(device))\n",
    "        dones = Variable(torch.from_numpy(np.vstack(self.experienceMemory['done'][Choosenindex]).astype(np.uint8)).float().to(device))\n",
    "        \n",
    "        \n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones , weights)\n",
    "    \n",
    "    def Batchsample(self):\n",
    "        \"\"\"Samples the batch here \"\"\"\n",
    "        all_priorities = self.experienceMemory['Priority']\n",
    "        probs = self.GetProbs(all_priorities)\n",
    "        indeArr = np.arange(len(probs))\n",
    "        Choosenindex = np.random.choice(indeArr, self.batch_size,p=probs, replace = False)\n",
    "        return probs[Choosenindex] , Choosenindex\n",
    "    \n",
    "    def GetProbs(self, Pri):\n",
    "        \"\"\" Probabilities are defiend here\"\"\"\n",
    "        Numerator = np.power(Pri, self.alpha)\n",
    "        Demoninator = np.sum(Numerator)\n",
    "        Probs = Numerator / Demoninator\n",
    "        return Probs\n",
    "    \n",
    "    def updatePriorities(self,Error):\n",
    "        #print(self.selectedIdx , Error , np.size(Error) , np.size(self.selectedIdx))\n",
    "        self.experienceMemory['Priority'][self.selectedIdx] = Error.flatten()\n",
    "                                    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.experienceMemory)\n",
    "\n",
    "def handling_echo_bug(echo , size = 5000):\n",
    "    state = np.zeros(5000)\n",
    "    state[:len(echo)] = echo\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device which is used is  cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# in case of cuda errors\n",
    "# device =  \"cpu\"\n",
    "print(\"Device which is used is \" , device)\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, state_type,seed,signal = \"echo\", augment_states_T=3 ):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "            Beta is for sacling down the weights update due to the Priority Queues\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.Beta = 1\n",
    "        self.Gamma = GAMMA\n",
    "        self.augment_time =  augment_states_T\n",
    "\n",
    "        # Q-Network\n",
    "        if(signal == \"echo\"):\n",
    "            self.qnetwork_local = QNetworkCNN_1D_BATBOT(state_size, action_size, seed , augment_frames =  augment_states_T).to(device)\n",
    "            self.qnetwork_target = QNetworkCNN_1D_BATBOT(state_size, action_size, seed , augment_frames =  augment_states_T).to(device)\n",
    "        else:\n",
    "            print_slow(\"AI: \", delay =0)\n",
    "            print_slow(\"Wtf! r u doing \\U0001F620 I am dying here... \\n\")\n",
    "            print_slow(\"ME: \", delay =0)\n",
    "            print_slow(\"Sorry \\U0001F605 zoned out little .. was watching some lame anime.. \\U0001F62C \\n\")\n",
    "            print_slow(\"AI: \", delay =0)\n",
    "            print_slow(\"Adai Venna !!!.... mudidu vellaiya paru... \\n\")\n",
    "            print_slow(\"ME: \", delay =0)\n",
    "            print_slow(\"Seringa.. \\U0001F62C \\n\")\n",
    "            time.sleep(1)\n",
    "            self.qnetwork_local = QNetworkANN_BATBOT(state_size, action_size, seed , augment_frames =  augment_states_T).to(device)\n",
    "            self.qnetwork_target = QNetworkANN_BATBOT(state_size, action_size, seed , augment_frames =  augment_states_T).to(device)\n",
    "            \n",
    "            \n",
    "        print(\"the model is defined successfully\", self.qnetwork_target)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR , weight_decay= WeightDecay)\n",
    "        self.state_type = state_type\n",
    "        # Replay memory\n",
    "        #self.memory = PriorityReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE,ALPHA,state_type, seed)\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        #Priority = self.getError(state, action, reward, next_state,done)\n",
    "        #self.memory.add(state, action, reward, next_state, done,Priority)\n",
    "        #self, state, action, reward, next_state, done\n",
    "        self.memory.add(state, action, reward, next_state,done)\n",
    "        global Epislon\n",
    "        Epislon *= Decay\n",
    "        if(Epislon < min_Epislon):\n",
    "            Epislon = min_Epislon\n",
    "        beta = 1 - Epislon\n",
    "        #print(beta)\n",
    "        # alpha value for priority buffer\n",
    "        global ALPHA2\n",
    "        ALPHA2 *= Alphadecay\n",
    "        if(ALPHA2 < ALPHA_min):\n",
    "            ALPHA2 = ALPHA_min\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE*3:\n",
    "                #uncomment this for priority buffer\n",
    "                #experiences = self.memory.sample(ALPHA2,beta)\n",
    "                for _ in range(N_Update_per):\n",
    "                    experiences = self.memory.sample_idx()\n",
    "                    self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        \n",
    "        #state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        # if e is not None and e[1].done != True and \n",
    "        # self.memory[e[0]-1].done != True and e[0]-1 > 0 and e[0] < len(agent.memory.memory)-1])\n",
    "        if (len(self.memory.memory) >= 2 and (self.memory.memory[-1].done or self.memory.memory[-2].done) == False):\n",
    "            #print(np.dtype(state), np.size(state))\n",
    "            #print(len(self.memory.memory), [self.memory.memory[-2].state[0],self.memory.memory[-1].state[0],state])\n",
    "            \n",
    "            left_state = torch.from_numpy(np.array([self.memory.memory[-2].state[0],self.memory.memory[-1].state[0],state[0]]).astype(np.float32)).float().view(1,-1).to(device)\n",
    "            \n",
    "            right_state = torch.from_numpy(np.array([self.memory.memory[-2].state[1],self.memory.memory[-1].state[1],state[1]])).float().view(1,-1).to(device)\n",
    "            \n",
    "            \n",
    "            #print(state.size())\n",
    "            self.qnetwork_local.eval()\n",
    "            #print(np.size(state))\n",
    "            with torch.no_grad():\n",
    "                action_values = self.qnetwork_local(left_state, right_state)\n",
    "            #print(action_values)\n",
    "            self.qnetwork_local.train()\n",
    "            #for debuging\n",
    "            #check =    random.choice(np.arange(self.action_size))\n",
    "            #check2 = np.argmax(action_values.cpu().data.numpy())\n",
    "            #print(check.dtype , check2.dtype)\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() > eps:\n",
    "                return np.argmax(action_values.cpu().data.numpy())\n",
    "            else:\n",
    "                return random.choice(np.arange(self.action_size))\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "            \n",
    "                   \n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        \n",
    "        # uncomment this line if u wanna use priority Buffer\n",
    "        #states, actions, rewards, next_states, dones, weights = experiences\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        #print(states.size())\n",
    "        size_now = states.size()\n",
    "        #print(\"size_now\",size_now[0], states[:,:,0,:].size(),self.augment_time)\n",
    "        #  To Do .. code needs change Data set differs\n",
    "        # last one needs to be the length of the echo\n",
    "        left_states = states[:,:,0].view(size_now[0] ,  -1)\n",
    "        right_states = states[:,:,1].view(size_now[0] ,  -1)\n",
    "        left_next_states = next_states[:,:,0].view(size_now[0] ,  -1)\n",
    "        right_next_states = next_states[:,:,1].view(size_now[0] ,  -1)\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        #getting the maximum action from the Q table and applying it in the \n",
    "        #print(\"states size\" , next_states.size())\n",
    "        #------this for DDQN ---- uncomment this for DDQN-------------------------\n",
    "        Q_targets_next_action = self.qnetwork_local(left_next_states , right_next_states).detach().argmax(1).unsqueeze(1)\n",
    "        #print(Q_targets_next_action)\n",
    "        Q_targets_next = self.qnetwork_target(left_next_states , right_next_states).gather(1 , Q_targets_next_action)\n",
    "        Q_targets_next = Q_targets_next.detach()\n",
    "        # Compute Q targets for current states \n",
    "        # Compute Q targets for current states \n",
    "        #print(Q_targets_next.size())\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "#         print(actions)\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(left_states, right_states).gather(1, actions)\n",
    "        #print(\"expected\", Q_expected.size())\n",
    "        #print(\"expected\" , Q_expected , Q_targets)\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # change reduce to False for priority buffer\n",
    "        \n",
    "        #loss = F.smooth_l1_loss(Q_expected, Q_targets)\n",
    "        #print(loss.size())\n",
    "        \n",
    "        # this removed due to the swtching to normal replay buffer\n",
    "        #loss = loss * (weights).detach()\n",
    "        #loss = torch.mean(loss)\n",
    "        #loss has to be scaled scaled down by weights = (1 /(N*p(i)))^Beta\n",
    "        #loss = F.l1_loss(Q_expected , Q_targets)\n",
    "        \n",
    "        #print(\"The Loss\", loss)\n",
    "        # Minimize the loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "        \n",
    "#         ##-- for updating the priorities in the experience replay buffer------###\n",
    "#         with torch.no_grad():\n",
    "#             Q_targets_next_action = self.qnetwork_local(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "#             #print(Q_targets_next_action)\n",
    "#             Q_targets_next = self.qnetwork_target(next_states).gather(1 , Q_targets_next_action)\n",
    "#             Q_targets_next = Q_targets_next.detach()\n",
    "#             # Compute Q targets for current states \n",
    "#             Q_targets = rewards + (gamma * Q_targets_next * (1 - dones)).detach()\n",
    "\n",
    "#             # Get expected Q values from local model\n",
    "#             Q_expected = self.qnetwork_local(states).gather(1, actions).detach()\n",
    "\n",
    "#             errors = torch.abs(Q_expected - Q_targets).cpu().data.numpy()\n",
    "#             self.memory.updatePriorities(errors)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def ConvnumpyTotorch(self,states, actions, rewards, next_states, dones):\n",
    "        # converts the numpy to tensor variables\n",
    "        states = torch.from_numpy(np.array([states])).float().to(device)\n",
    "        actions = torch.from_numpy(np.array([[actions]])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.array([rewards])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.array([next_states])).float().to(device)\n",
    "        dones = torch.from_numpy(np.array([dones]).astype(np.uint8)).float().to(device)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "        \n",
    "    def getError(self,states, actions, rewards, next_states, dones):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #print(\"states\" , states.shape , next_states.shape)\n",
    "            states, actions, rewards, next_states, dones = self.ConvnumpyTotorch(states, actions, rewards, next_states, dones)\n",
    "            #print(\"states\",states , next_states)\n",
    "            Q_targets_next_action = self.qnetwork_local(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "            #print(\"actions\",Q_targets_next_action , actions)\n",
    "            #print(next_states.shape)\n",
    "            Q_targets_next = self.qnetwork_target(next_states).gather(1 , Q_targets_next_action)\n",
    "            Q_targets_next = Q_targets_next.detach()\n",
    "            # Compute Q targets for current states \n",
    "            Q_targets = rewards + (GAMMA * Q_targets_next * (1 - dones))\n",
    "\n",
    "            # Get expected Q values from local model\n",
    "\n",
    "            Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "            Q_expected = Q_expected.detach()\n",
    "            #error =  F.l1_loss(Q_expected , Q_targets)\n",
    "            error =  torch.abs(Q_expected - Q_targets).cpu().data.numpy()\n",
    "        return (error.item() + ErrorOffset)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_obs , echo_time  = env.get_observationEnv() \n",
    "# print(len(test_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Wtf! r u doing 😠 I am dying here... \n",
      "ME: Sorry 😅 zoned out little .. was watching some lame anime.. 😬 \n",
      "AI: Adai Venna !!!.... mudidu vellaiya paru... \n",
      "ME: Seringa.. 😬 \n",
      "the model is defined successfully QNetworkANN_BATBOT(\n",
      "  (fc1): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (fc1bnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc2bnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3bnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(len(observation), 3 , state_Type , seed = 0 , signal = \"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.9025972751814084, 35.628467904048186)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_episode = 150\n",
    "next_saving = 50\n",
    "saving_score = 1.5\n",
    "next_saving_score = 1\n",
    "norm_max = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: -2.32Maxi 231\n",
      "Episode 2\tAverage Score: -2.49Maxi 299\n",
      "Episode 3\tAverage Score: -2.61Maxi 299\n",
      "Episode 4\tAverage Score: -2.70Maxi 299\n",
      "Episode 5\tAverage Score: -2.74Maxi 299\n",
      "Episode 6\tAverage Score: -2.61Maxi 299\n",
      "Episode 7\tAverage Score: -2.64Maxi 299\n",
      "Episode 8\tAverage Score: -2.66Maxi 299\n",
      "Episode 9\tAverage Score: -2.68Maxi 299\n",
      "Episode 10\tAverage Score: -2.64Maxi 299\n",
      "Episode 10\tAverage Score: -2.64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 11\tAverage Score: -2.67Maxi 299\n"
     ]
    }
   ],
   "source": [
    "# have to modify this according to the Gazebo sim\n",
    "# did that\n",
    "def dqn(n_episodes=3000, max_t=10000, eps_start=1.0, eps_end=0.01, eps_decay=0.99):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    avg_scores = []\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start# initialize epsilon\n",
    "    Max_t = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        global saving_score\n",
    "        global saving_episode\n",
    "        if np.mean(scores_window)>= saving_score or (i_episode % saving_episode == 0 and i_episode != 0):\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint_agent_at_'+str(i_episode)+'.pth')\n",
    "            saving_episode += next_saving\n",
    "            if np.mean(scores_window)>= saving_score:\n",
    "                saving_score += next_saving_score\n",
    "            if np.mean(scores_window)>=160.0:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        \n",
    "        state = env.reset()\n",
    "#         # just using the first 5000 values of the echo sequence\n",
    "#         if len(observation[0]) >= 5000:\n",
    "#             state = [observation[0][:5000]/norm_max, observation[1][:5000]/norm_max]\n",
    "#         else :\n",
    "#             state = [handling_echo_bug(observation[0])/norm_max, handling_echo_bug(observation[1])/norm_max]\n",
    "       \n",
    "        score = 0\n",
    "        #print(\"hello\")\n",
    "        for t in range(max_t):\n",
    "            #print(\"state\",state[0].shape)\n",
    "            action = agent.act(state, eps)\n",
    "            #print(action.dtype)\n",
    "            action = np.int32(action)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            #assert(len(next_observation[0])> 5000), \"wtf fuck is going on \"+str(next_observation[0].shape)\n",
    "#             if len(next_observation[0]) >= 5000:\n",
    "#                 next_state = [next_observation[0][:5000]/norm_max , next_observation[1][:5000]/norm_max]\n",
    "#             else :\n",
    "#                 next_state = [handling_echo_bug(next_observation[0])/norm_max, handling_echo_bug(next_observation[1])/norm_max]\n",
    "            #print(\"next_state\",next_state[0].shape, len(next_observation), next_observation[0].shape )\n",
    "            #next_state = np.resize(extract_luminance_state(env_info.visual_observations[0], peek = True) , (1,84,84))\n",
    "            #print(next_state.shape)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            #print(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                Max_t = t\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        print(\"Maxi\" ,Max_t)\n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            avg_scores.append(np.mean(scores_window))\n",
    "            plt.plot(np.arange(len(scores)), scores)\n",
    "            plt.ylabel('Score')\n",
    "            plt.xlabel('Episode #')\n",
    "            plt.show()\n",
    "            \n",
    "        if i_episode % 100 == 0 and i_episode != 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            plt.plot(np.arange(len(avg_scores)), avg_scores)\n",
    "            plt.ylabel('Score')\n",
    "            plt.xlabel('Episode #')\n",
    "            plt.show()\n",
    "        \n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(saving_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
